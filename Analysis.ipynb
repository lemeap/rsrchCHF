{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic environment modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2 as pg # PostgreSQL module\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "# Data analysis modules\n",
    "from CoolProp.CoolProp import PropsSI\n",
    "from CoolProp.CoolProp import Props\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, PolynomialFeatures, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.integrate import ode\n",
    "from scipy.interpolate import griddata\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import sympy as sp\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "# Database modules\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# User-defined libraries\n",
    "from Model import *\n",
    "from PhysicalProperty import *\n",
    "from StructuredQuery import *\n",
    "from Numeric import *\n",
    "from Authentication import *\n",
    "\n",
    "# Set the display format to show 6 decimal places\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "print(\"Import modules successfully.\")\n",
    "\n",
    "# Add this line before the loop that iterates over the groups\n",
    "warnings.filterwarnings('ignore', category=ValueWarning)\n",
    "\n",
    "# load class instances\n",
    "sql = StructuredQuery()\n",
    "pro = PhysicalProperty()\n",
    "mod = Model()\n",
    "oau = Authentication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# Load json file (Security)\n",
    "json_parse = oau.get_apikey(json_filename='key.json')\n",
    "json_res = pd.json_normalize(json_parse['sql'])\n",
    "\n",
    "# Set query from DB\n",
    "sql = \"SELECT * FROM CHF.new_res_chf_tb WHERE geo = 'A'\"\n",
    "\n",
    "# Connect DB server\n",
    "#(conn, db_engine) = sql.connect(json_res[\"host\"][1], json_res[\"dbname\"][1], json_res[\"user\"][1], json_res[\"port\"][1], json_res[\"password\"][1], json_res[\"service\"][1]) #postgreSQL\n",
    "connect = pys.connect(host=json_res[\"host\"][1], user = json_res[\"user\"][1], password = json_res[\"password\"][1], cursorclass=pys.cursors.DictCursor)\n",
    "cur = connect.cursor()\n",
    "cur.execute(sql)\n",
    "\n",
    "result = cur.fetchall()\n",
    "prop_tb = pd.DataFrame(result)\n",
    "\n",
    "del result, sql\n",
    "\n",
    "# Set query from DB\n",
    "sql = \"SELECT * FROM CHF.physicalproperties\"\n",
    "cur.execute(sql)\n",
    "result = cur.fetchall()\n",
    "pp_tb = pd.DataFrame(result)\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "cur.close()\n",
    "del result, sql, connect # delete quiry log\n",
    "\n",
    "print(\"Loading database completed. data size: {}\".format(len(prop_tb)))\n",
    "print(\"Loading database completed. data size: {}\".format(len(pp_tb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the summary of dataframe\n",
    "print(len(prop_tb))\n",
    "prop_tb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "\n",
    "# Filter the dataframe based on the condition Xt > 0\n",
    "prop_tb = prop_tb[(prop_tb['cal_xt'] > 0) & (prop_tb['refri'] != 'He') & (prop_tb['xi'] > -1) & (prop_tb['q'] < 20)].copy()\n",
    "\n",
    "for i, row in prop_tb.iterrows():\n",
    "    prop_tb.loc[i, 'param'] = ((prop_tb.loc[i, 'cpv']*prop_tb.loc[i, 'muv'])/prop_tb.loc[i,'kf'])*(prop_tb.loc[i, 'rhov']/prop_tb.loc[i, 'rhof'])\n",
    "    if prop_tb.loc[i, 'geo'] == \"C\":\n",
    "        prop_tb.loc[i, 'bta'] = (1-prop_tb.loc[i, 'cal_xosv'])/((prop_tb.loc[i, 'qt'])**(1-prop_tb.loc[i, 'dh']))\n",
    "        prop_tb.loc[i, 'rdmh'] = 1\n",
    "        prop_tb.loc[i, 'geop'] = 1\n",
    "    else:\n",
    "        prop_tb.loc[i, 'bta'] = (1-prop_tb.loc[i, 'cal_xosv'])/(prop_tb.loc[i, 'qt'])**(1-prop_tb.loc[i, 'dio']/prop_tb.loc[i, 'doi'])\n",
    "        prop_tb.loc[i, 'rdmh'] = (prop_tb.loc[i, 'doi']**2-prop_tb.loc[i, 'dio']**2)/(2*np.log(prop_tb.loc[i, 'doi']/prop_tb.loc[i, 'dio']))\n",
    "        prop_tb.loc[i, 'geop'] = (prop_tb.loc[i, 'doi']/prop_tb.loc[i, 'dio'])*(prop_tb.loc[i, 'rdmh']-prop_tb.loc[i, 'dio']**2)/(prop_tb.loc[i, 'doi']**2-prop_tb.loc[i, 'rdmh'])\n",
    "    prop_tb.loc[i, 'kpa'] = 1/prop_tb.loc[i, 'dh']\n",
    "\n",
    "    if prop_tb.loc[i, 'xi'] < prop_tb.loc[i, 'cal_xosv']:\n",
    "        prop_tb.loc[i, 'vf_alpha'] = 1/(1+prop_tb.loc[i, 'bta']*np.exp(-prop_tb.loc[i, 'kpa']*prop_tb.loc[i, 'cal_xosv']))\n",
    "    else:\n",
    "        prop_tb.loc[i, 'vf_alpha'] = 1/(1+prop_tb.loc[i, 'bta']*np.exp(-prop_tb.loc[i, 'kpa']*prop_tb.loc[i, 'xi']))\n",
    "    prop_tb.loc[i, 'mass'] = round(PropsSI(prop_tb.loc[i, 'refri'], 'molemass') * 1e3,6)\n",
    "    prop_tb.loc[i, 'de'], prop_tb.loc[i, 'ph'], prop_tb.loc[i,'af'] = pro.cal_de(prop_tb.loc[i, 'geo'], prop_tb.loc[i, 'hs'], prop_tb.loc[i, 'doi'], prop_tb.loc[i, 'dio'], prop_tb.loc[i, 'dh'], prop_tb.loc[i, 'lh'])\n",
    "    prop_tb.loc[i,'tcrit'] = round(PropsSI('Tcrit', prop_tb.loc[i, 'refri']), 12)\n",
    "    \n",
    "# Drop missing values from the group DataFrame\n",
    "prop_tb = prop_tb.dropna(subset=['vf_alpha'])\n",
    "\n",
    "y0 = 0.375\n",
    "xc = 0.795\n",
    "aa =  1e-4\n",
    "wg = 0.5\n",
    "wl = 0.2\n",
    "mu = 1000\n",
    "\n",
    "y00 = 1.25\n",
    "a0 = -2\n",
    "xcc = -4\n",
    "ww = 8.5\n",
    "a0 = -2\n",
    "a1 = -0.75\n",
    "a2 = 1.25\n",
    "\n",
    "prop_tb['qcosr'] = 8.314*prop_tb['tcrit']*prop_tb['g']/(prop_tb['mass']*1e3)\n",
    "prop_tb['gcosr'] = 8.314*prop_tb['pcrit']*prop_tb['v']*prop_tb['mass']/(prop_tb['tcrit'])\n",
    "prop_tb['zxtt'] = (1+prop_tb['vf_alpha']+prop_tb['cal_xt']**2)**3\n",
    "prop_tb['j_alpha'] = 1.8 - 1.85*(np.abs(prop_tb['rdcp'] - 0.8)**2)\n",
    "#prop_tb['j_alpha'] = 1.5 + -15.657*prop_tb['rdcp'] + 107.92*prop_tb['rdcp']**2 + -300.6*prop_tb['rdcp']**3 + 362.85*prop_tb['rdcp']**4 + -159.39*prop_tb['rdcp']**5\n",
    "#prop_tb['j_beta'] = (0.275 + 0.5*np.log(prop_tb['rdcp'])**2 + (2*0.125/3.141592)*(0.0775/(4*(np.log(prop_tb['rdcp'])-0.675)**2 + 0.0775**2)))\n",
    "prop_tb['j_beta'] = y0 + aa * ( mu * (1/3.141592) *(wl / (25*(np.sqrt(prop_tb['rdcp'])-xc)**2 + wl**2)) + (1 - mu) * (np.sqrt(2*np.log(2)) / (np.sqrt(3.141592) * wg)) * np.exp(-(5*np.log(2)/wg**2)*(np.sqrt(prop_tb['rdcp'])-xc)**2) )\n",
    "#prop_tb['j_beta'] = 1413.5*prop_tb['rdcp']**6 + -3602.3*prop_tb['rdcp']**5 + 3429.7*prop_tb['rdcp']**4 + -1516.5*prop_tb['rdcp']**3 + 319.25*prop_tb['rdcp']**2 + -29.234*prop_tb['rdcp'] + 1.6076\n",
    "prop_tb['X'] = -(1*np.sqrt(prop_tb['gcosr'])/(1+prop_tb['ph']*prop_tb['lh']))\n",
    "prop_tb['Y'] = np.log(prop_tb['qt']*np.sqrt(prop_tb['dh']/(prop_tb['lam']*prop_tb['geop']))\n",
    "\n",
    "prop_tb['Xz'] = (1/np.sqrt(prop_tb['dh']**prop_tb['geop']))*np.exp(-1*np.sqrt(prop_tb['gcosr']*prop_tb['cal_xt']*prop_tb['zxt']))\n",
    "prop_tb['Yz'] = prop_tb['qt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1 by 2 subplot\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 10))\n",
    "\n",
    "c = prop_tb['pr']\n",
    "\n",
    "# Plot intercept vs rdcp in the first subplot\n",
    "ax1.scatter(prop_tb['Xz'], prop_tb['Yz'], c = c, edgecolors= 'Black', marker='o')\n",
    "ax1.plot(prop_tb['Yz'], prop_tb['Yz'], color = 'red')\n",
    "ax1.plot(prop_tb['Yz'], prop_tb['Yz']*0.65, linestyle= '-', color = 'blue')\n",
    "ax1.plot(prop_tb['Yz'], prop_tb['Yz']*1.35, linestyle= '-', color = 'blue')\n",
    "ax1.set_xlabel('X [-]')\n",
    "ax1.set_ylabel('Y')\n",
    "#ax1.set_xlim(0, 2)\n",
    "#ax1.set_ylim(0, 2)\n",
    "\n",
    "# Plot coefficient vs rdcp in the second subplot\n",
    "ax2.scatter(prop_tb['rdcp'], prop_tb['j_alpha'], color='White', edgecolors= 'Black', marker='o')\n",
    "#ax2.plot(prop_tb['Y'], prop_tb['Y'], color = 'red')\n",
    "ax2.set_xlabel('X [-]')\n",
    "ax2.set_ylabel('Beta')\n",
    "\n",
    "# Plot coefficient vs rdcp in the second subplot\n",
    "ax3.scatter((prop_tb['rdcp']), prop_tb['j_beta'], color='White', edgecolors= 'Black', marker='o')\n",
    "ax3.set_xlabel('Reduced pressure [-]')\n",
    "ax3.set_ylabel('Beta')\n",
    "\n",
    "# Plot coefficient vs rdcp in the second subplot\n",
    "ax4.scatter(prop_tb['X'], prop_tb['Y'], edgecolors= 'Black', c = c, marker='o', cmap='rainbow')\n",
    "#fig.colorbar(ax4)  # Add colorbar for reference\n",
    "#ax4.colorbar()  # Add colorbar for reference\n",
    "ax4.set_xlabel('Reduced pressure [-]')\n",
    "ax4.set_ylabel('Alpha')\n",
    "ax4.set_xlim(-8, 0)\n",
    "ax4.set_ylim(-8, 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Determine the parameter\n",
    "XX = (prop_tb['rdcp'])\n",
    "\n",
    "# set bins\n",
    "bin_size = 0.01  # If you want to adjust the size of the range, change this value\n",
    "bins = pd.interval_range(start=round(XX.min() - bin_size / 2,5), end=round(XX.max() + bin_size / 2,5), freq=bin_size)\n",
    "prop_tb['x_range'] = pd.cut(round(XX, 2), bins)\n",
    "\n",
    "grouped_data = prop_tb.groupby('x_range')\n",
    "\n",
    "# 각 그룹의 평균 y 값을 계산하고 내림차순으로 정렬\n",
    "group_mean = grouped_data['rdcp'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Assign index numbers for labeling\n",
    "group_labels = {group: f'G{idx + 1}' for idx, group in enumerate(group_mean.index)}\n",
    "\n",
    "# Extract the range of each group\n",
    "group_ranges = [(group, group.left, group.right) for group in grouped_data.groups]\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "group_ranges_df = pd.DataFrame(group_ranges, columns=['x_range', 'x_min', 'x_max'])\n",
    "\n",
    "# group_labels 딕셔너리를 사용하여 각 행에 해당하는 라벨 찾기\n",
    "prop_tb['group_label'] = prop_tb['x_range'].apply(lambda x: group_labels[x])\n",
    "\n",
    "# Calculate the number of groups\n",
    "num_groups = len(grouped_data)\n",
    "\n",
    "# Calculate the number of columns needed for the subplots\n",
    "num_columns = math.ceil(num_groups / 10)\n",
    "\n",
    "# Create subplots for each group in a 10 x n grid\n",
    "fig, axes = plt.subplots(nrows=10, ncols=num_columns, figsize=(5 * num_columns, 30), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create an empty DataFrame to store the summary results\n",
    "summary_prop_tb = pd.DataFrame(columns=['Group', 'Intercept'])\n",
    "\n",
    "# Perform partial linear regression for each group\n",
    "for ax, (group_name, group_prop_tb) in zip(axes, grouped_data):\n",
    "    \n",
    "    # Fit the model without the variable to be partially regressed out\n",
    "    if len(group_prop_tb) < 5:\n",
    "        pass\n",
    "    else:\n",
    "        partial_model = smf.ols(formula='Y ~ X', data=group_prop_tb).fit()\n",
    "    \n",
    "        # Calculate the confidence intervals for each observation\n",
    "        predictions = partial_model.get_prediction(group_prop_tb)\n",
    "        conf_int = predictions.conf_int(alpha=0.05)  # 95% confidence level\n",
    "        \n",
    "        # Check the shape of conf_int before slicing it\n",
    "        if conf_int.shape[1] == 1:\n",
    "            # reshape conf_int to have two columns\n",
    "            conf_int = np.hstack([conf_int, conf_int])\n",
    "        \n",
    "        # Filter the group data to include only observations with a 95% confidence level\n",
    "        in_confidence_interval = (conf_int[:, 0] <= group_prop_tb['Y']) & (group_prop_tb['Y'] < conf_int[:, 1])\n",
    "        filtered_group_prop_tb = group_prop_tb[in_confidence_interval]\n",
    "        \n",
    "        # Perform the partial regression by regressing the residuals on the remaining independent variable(s)\n",
    "        residuals = partial_model.resid[in_confidence_interval]\n",
    "        filtered_group_prop_tb = filtered_group_prop_tb.assign(residuals=residuals)\n",
    "\n",
    "        # Append the partial regression results for the current group to the summary DataFrame\n",
    "        val_g = filtered_group_prop_tb['g'].mean()\n",
    "        val_re = filtered_group_prop_tb['re'].mean()\n",
    "        val_pr = filtered_group_prop_tb['pr'].mean()\n",
    "        val_pe = filtered_group_prop_tb['pe'].mean()\n",
    "        val_rdcp = filtered_group_prop_tb['rdcp'].mean()\n",
    "        val_dh = filtered_group_prop_tb['dh'].mean()\n",
    "        val_param = filtered_group_prop_tb['param'].mean()\n",
    "        val_lh = filtered_group_prop_tb['lh'].mean()\n",
    "        val_cosr = filtered_group_prop_tb['qcosr'].mean()\n",
    "        val_gcosr = filtered_group_prop_tb['gcosr'].mean()\n",
    "        intercept = partial_model.params['Intercept']\n",
    "        coeff = partial_model.params['X']\n",
    "        r_squared = partial_model.rsquared\n",
    "        f_statistic = partial_model.fvalue\n",
    "        p_value = partial_model.f_pvalue\n",
    "        num_of_data_points = len(filtered_group_prop_tb)\n",
    "        summary_prop_tb = pd.concat([summary_prop_tb, pd.DataFrame({'Group': [group_name], 'Intercept': [intercept], 'Coefficient': [coeff], 'R-squared': [r_squared], 'F-statistic': [f_statistic], 'p-value': [p_value], 'No': [num_of_data_points], 'mean_param': [val_param],'mean_dh': [val_dh], 'mean_lh': [val_lh], 'mean_cosr': [val_cosr], 'mean_gcosr': [val_gcosr], 'mean_rdcp': [val_rdcp], 'mean_re': [val_re], 'mean_g': [val_g], 'mean_pr': [val_pr]})], ignore_index=True)\n",
    "\n",
    "        # Plot the partial linear regression results for each group on the subplot\n",
    "        ax.scatter(filtered_group_prop_tb['rdcp'], filtered_group_prop_tb['residuals'], label='Data')\n",
    "        #ax.plot(filtered_group_prop_tb['rdcp'], partial_model.fittedvalues, color='red', label='Fitted Line')\n",
    "        ax.set_xlabel('Dependent_value')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        ax.set_title(f'Partial Linear Regression for Group {group_name}')\n",
    "        ax.legend()\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(num_groups, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Display the subplots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary DataFrame with partial regression coefficients, intercepts, statistical results, and number of data points for each group\n",
    "summary_prop_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract p value is under 0.05\n",
    "summary_tb = summary_prop_tb[(summary_prop_tb['p-value'] <= 0.005) & (summary_prop_tb['R-squared'] > 0.90) & (summary_prop_tb['No'] > 5)].copy()\n",
    "\n",
    "# Extract intercept, coefficient and rdcp from the summary_prop_tb DataFrame\n",
    "intercepts = summary_tb['Intercept']\n",
    "coefficients = summary_tb['Coefficient']\n",
    "val_x = ((summary_tb['mean_rdcp']))\n",
    "\n",
    "# Create a 1 by 2 subplot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot intercept vs rdcp in the first subplot\n",
    "ax1.scatter(val_x, intercepts)\n",
    "ax1.set_xlabel('Reduced Pressure [-]')\n",
    "ax1.set_ylabel('Intercept')\n",
    "\n",
    "# Plot coefficient vs rdcp in the second subplot\n",
    "ax2.scatter(val_x, coefficients)\n",
    "ax2.set_xlabel('Reduced Pressure [-]')\n",
    "ax2.set_ylabel('Coefficient')\n",
    "\n",
    "plt.show()\n",
    "summary_tb['lnIntercept'] = summary_tb.apply(lambda x: np.exp(x['Intercept']), axis=1)\n",
    "summary_tb.to_csv('summary_prop_tb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_group_prop_tb.rdcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans Algorithm\n",
    "# Preparing converged dataset\n",
    "val_tb = prop_tb[(prop_tb['yn_hval'] == 100) & (prop_tb['idx'] != 31) & (prop_tb['xi'] > -1) & (prop_tb['idx'] != 25) & (abs(prop_tb['qval']) < 0.5)].copy()\n",
    "\n",
    "# Preparing linear form of CHF correlation\n",
    "X = val_tb[['p']]\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# Scaling\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Calculate density of each sample\n",
    "no_k = 100\n",
    "wcss = []\n",
    "\n",
    "for k in range(1, no_k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, max_iter=1000, n_init=50)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the WCSS values as a function of the number of clusters\n",
    "plt.plot(range(1, no_k), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "from kneed import KneeLocator\n",
    "# Find the elbow point in the WCSS curve\n",
    "kneedle = KneeLocator(range(1, no_k), wcss, curve = 'convex', direction='decreasing')\n",
    "elbow_point = kneedle.elbow\n",
    "print(elbow_point)\n",
    "\n",
    "# Run K-means clustering for elbow point\n",
    "kmeans = KMeans(n_clusters=elbow_point, random_state=0, max_iter=1000, n_init=1).fit(X, sample_weight=1)\n",
    "\n",
    "# Extract the labels (cluster assignments) and centroids\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Add the labels and centroids to the original DataFrame\n",
    "val_tb['cluster'] = labels\n",
    "val_tb['centroid_distance'] = kmeans.transform(X).min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique groups\n",
    "groups = val_tb.sort_values('cluster')['cluster'].unique()\n",
    "\n",
    "# calculate the number of rows and columns\n",
    "n_groups = len(groups)\n",
    "n_cols = 3\n",
    "n_rows = (n_groups + n_cols - 1) // n_cols\n",
    "\n",
    "# create a grid of subplots\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*3, n_rows*3))\n",
    "\n",
    "# plot data for each group\n",
    "for i, group in enumerate(groups):\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    ax = axs[row, col]\n",
    "    ax.scatter(val_tb.loc[val_tb['cluster'] == group, 'rdcp'], \n",
    "                val_tb.loc[val_tb['cluster'] == group, 'q'], \n",
    "                label=group)\n",
    "    ax.set_xlabel('XXX')\n",
    "    ax.set_ylabel('YYY')\n",
    "    ax.set_title('Label: '+ str(group))\n",
    "\n",
    "# adjust the spacing between subplots\n",
    "plt.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95,\n",
    "                    wspace=0.5, hspace=0.5)\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN algorithm\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preparing linear form of CHF correlation\n",
    "X = val_tb[['rdcp']]\n",
    "\n",
    "scaler = QuantileTransformer()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Run DBSCAN algorithm\n",
    "dbscan = DBSCAN(eps=0.00001, min_samples = 5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X['rdcp'], val_tb['q'], c=labels, cmap ='gist_rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering grouping and calculation for loop\n",
    "\n",
    "# normalize the dataframe using MinMax Scaler\n",
    "def cal_mnmx(df, cols):\n",
    "    # create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # fit the scaler to the dataframe\n",
    "    scaler.fit(df[cols])\n",
    "    # transform the selected columns of the dataframe using the scaler\n",
    "    res = pd.DataFrame(scaler.transform(df[cols]), columns=cols)\n",
    "\n",
    "    return res\n",
    "\n",
    "from sklearn.metrics import r2_score as r2\n",
    "# define a function to perform linear regression on a group\n",
    "def cal_lr(df):\n",
    "    # create a linear regression object\n",
    "    model = LinearRegression()\n",
    "    cluster_value = len(df['cluster'])\n",
    "    X = df[['X_zxt']].values.reshape(-1,1)\n",
    "    y = df['Y_qcal'].values.reshape(-1,1)\n",
    "\n",
    "    # fit the model to the data\n",
    "    res_model = model.fit(X, y)\n",
    "    r2_score=r2(y, res_model.predict(X))\n",
    "\n",
    "    # return the slop and intercept of the regression line\n",
    "    return pd.Series({'no': cluster_value, 'g_alpha': res_model.intercept_[0], 'g_gamma': res_model.coef_[0][0], 'r2_score': r2_score})\n",
    "\n",
    "# group the DataFrame by the 'cluster' column\n",
    "#val_group = val_tb.groupby('cluster').apply(cal_lr)\n",
    "val_group_tb = val_tb.groupby('cluster').apply(cal_lr).reset_index()\n",
    "val_group_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two dataframe along rows\n",
    "result = pd.merge(val_tb, val_group_tb, left_on='cluster', right_on = val_group_tb['cluster'], how='outer').copy()\n",
    "#result2 = pd.merge(result, tmp_res, on = result.index, how='outer').copy()\n",
    "\n",
    "# 이 부분은 함수로 데이터프레임에 적용할 수 있도록 수정해야 함.\n",
    "lr = LinearRegression()\n",
    "\n",
    "\n",
    "# Set the X and Y data\n",
    "X = result[['rdcp']]\n",
    "Y = result['g_alpha']\n",
    "\n",
    "# Perform polynomial regression\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_poly = poly.fit_transform(X)\n",
    "lr.fit(X_poly, Y)\n",
    "r2_train = r2(Y, lr.predict(X_poly))\n",
    "\n",
    "print('Coefficients: ', lr.coef_)\n",
    "print('Intercept:', round(lr.intercept_,6))\n",
    "print('R2: ', round(r2_train*100,2))\n",
    "\n",
    "# Plot graph\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(4, 4, 1)\n",
    "ax2 = fig.add_subplot(4, 4, 2)\n",
    "ax3 = fig.add_subplot(4, 4, 3)\n",
    "ax4 = fig.add_subplot(4, 4, 4)\n",
    "\n",
    "plt.xlabel('Reduced Pressure [-]')\n",
    "plt.ylabel('Alpha or Gamma')\n",
    "plt.legend(loc='upper left')\n",
    "ax1.scatter(result.loc[:, 'rdcp'], result.loc[:, 'g_alpha'], c=result['dh'], cmap ='gist_rainbow')\n",
    "ax2.scatter(result.loc[:, 'rdcp'], result.loc[:,'g_gamma'], c=result['p'], cmap ='gist_rainbow')\n",
    "ax3.scatter((result.loc[:, 'X_zxt']), (result.loc[:,'Y_qcal'])/result.loc[:,'g_alpha'], c=result['rdcp'], cmap ='gist_rainbow')\n",
    "ax4.scatter(result.loc[:,'g_alpha'], result.loc[:,'g_gamma'], c=result['rdcp'], cmap ='gist_rainbow')\n",
    "\n",
    "#ax1.plot(xs,ys1,'r-',lw=3)\n",
    "#ax2.plot(xs,ys2,'b-',lw=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('research': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e4efffbaf51c39b67b2ae8f1226cd2ee0dd9610494cee8a8182e97691ddb7ade"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
